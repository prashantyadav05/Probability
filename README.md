# Probability
* For a random variable x, P(x) is a function that assigns a probability to all possible values of x.          
* **Joint Probability:** Occurence of 2 or more events.
* **Marginal Probability:** Marginal probability is the probability of an event irrespective of the outcome of other variables. Sum of joint probabilities.
* **Conditional Probability:** Ratio of joint probability of the event A and B, to the marginal probability of event B.



## Information Entropy
**Information theory** is the quantification of the amount of information in things like events, random variables, and distributions. Quantifying the amount of information requires the use of probabilities, hence the relationship of information theory to probability.
* Low Probability Event: High Information (surprising).
* High Probability Event: Low Information (unsurprising).
## Divergence Between Probability Distributions
### Statistical Distance
### Kullback-Leibler Divergence
KL divergence score, quantifies how much one probability distribution diers from another probability distribution. The KL divergence
between two distributions Q and P is often stated using the following notation:
### Jensen-Shannon Divergence
## Cross-Entropy for Machine Learning
## Information Gain and Mutual Information
