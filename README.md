# Probability




## Information Entropy
**Information theory** is the quantification of the amount of information in things like events, random variables, and distributions. Quantifying the amount of information requires the use of probabilities, hence the relationship of information theory to probability.
* Low Probability Event: High Information (surprising).
* High Probability Event: Low Information (unsurprising).
## Divergence Between Probability Distributions
### Statistical Distance
### Kullback-Leibler Divergence
KL divergence score, quantifies how much one probability distribution diers from another probability distribution. The KL divergence
between two distributions Q and P is often stated using the following notation:
### Jensen-Shannon Divergence
## Cross-Entropy for Machine Learning
## Information Gain and Mutual Information
